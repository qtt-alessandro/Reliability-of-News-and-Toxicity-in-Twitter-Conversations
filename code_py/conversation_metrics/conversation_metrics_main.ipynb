{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "import random \n",
    "import pandas as pd \n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from test_backbone import * \n",
    "from tree_metrics  import *\n",
    "from annotations_processing import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlets = ['rt_com', 'dailyherald', 'parentsmagazine', 'TheGoodGodAbove', 'bright_side_me', 'TheMarySue', 'ChinaDailyUSA', 'nbc6', 'MediaTakeoutTV', 'NationalMemo', 'newsandguts', 'theinquisitr', 'VoiceofPD', 'mercola', 'CBSLA', 'veteranstoday', 'NewsOn6', 'thedailybanter', 'unhealthytruth', 'Greg_Palast', 'lifebiomedguru', 'V_of_Europe', 'voguemagazine', 'TheOhioStar', 'MadWorldNews', 'PoliTribune', 'strange_sounds', 'realdennislynch', 'NatEnquirer', 'thrive', 'PanData19','ascienthusiast', 'BIZPACReview', 'FoxNews', 'drchrisnorthrup', 'healthychildren', 'NewsBecker', 'chicksonright', 'USATODAY', 'WayneDupreeShow', 'scarymommy', 'EpochTimes', 'ebonymag', 'NYDailyNews', 'twpundit', 'thetnstar', 'houstonpress', 'WGNRadio', 'nypost', 'tassagency_en', 'wearemitu', 'percolately', 'GeorgiaStarNews', 'mindys4Biden', 'esquire', 'KyivIndependent', 'digg', 'nra', 'voxdotcom', 'CNN', 'FDRLST', 'nytimes', 'BreitbartNews', 'KyivPost', 'SputnikInt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlets = ['CNN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ng = pd.read_csv('/Users/alessandroquattrociocchi/Documents/data/NewsGuard/Countries/USA_newsguard_handle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 157/1500 [00:07<00:37, 36.25it/s]"
     ]
    }
   ],
   "source": [
    "get_results_dict = defaultdict(list)\n",
    "overall_unique_users = []\n",
    "overall_toxic_authors = []\n",
    "toxicity_threshold = 0.60\n",
    "path = '/Users/alessandroquattrociocchi/Documents/Data/Twitter/comments_labelled_newsguard/comments_all_evaluated/'\n",
    "\n",
    "tree_metrics = Metrics()\n",
    "\n",
    "for outlet in (outlets):\n",
    "    \n",
    "    all_files = glob.glob(os.path.join(path + str(outlet) , \"*.csv.xz\"))\n",
    "    for filename in tqdm(all_files):\n",
    "        s = filename.split('/')\n",
    "        tweet_id = s[-1][:-7]\n",
    "        df = pd.read_csv(filename, index_col=None, header=0, low_memory=False,dtype=str)\n",
    "        \n",
    "        ##\n",
    "        df = PrePreprocessing.adjust_columns_name(df)\n",
    "        df = PrePreprocessing.preprocessing_df(df, filter_na = True)\n",
    "        toxic_df = PrePreprocessing.filter_toxic_comments(df, tox_threshold = 0.6)\n",
    "        root_node = PrePreprocessing.get_root(df)\n",
    "\n",
    "        overall_unique_users += list(set(df.author_id.tolist() + df.in_reply_to_user_id.tolist()))\n",
    "        overall_toxic_authors += toxic_df[\"author_id\"].tolist()\n",
    "\n",
    "        #retrieving the tweet identifier\n",
    "        get_results_dict['tweet_id'].append(root_node)\n",
    "        #retrieving the outlet's name\n",
    "        get_results_dict['outlet_name'].append(outlet)\n",
    "        #retrieving the outlet's score\n",
    "        get_results_dict['outlet_score'].append(ng[ng['Twitter Handle'] == outlet].Score.values[0])\n",
    "        get_results_dict['outlet_label'].append(PrePreprocessing.get_label(ng[ng['Twitter Handle'] == outlet].Score.values[0]))\n",
    "        #retrieving the outlet's flag\n",
    "        get_results_dict['outlet_flag'].append(ng[ng['Twitter Handle'] == outlet].Rating.values[0])\n",
    "        #retrieving the unique users\n",
    "        get_results_dict['unique_users'].append(len(set(df.author_id.tolist() + df.in_reply_to_user_id.tolist())))\n",
    "        #retrieving the first comment datetime\n",
    "        get_results_dict['created_at'].append(df.created_at.iloc[0])\n",
    "        #get the total number of comments in the conversation thread -> tree size = number of nodes in the graph\n",
    "        get_results_dict['n_comments'].append(len(df))\n",
    "        #get the total number of toxic comments, i.e. the number of comments that exceed the threshold\n",
    "        get_results_dict['n_tox_comments'].append(len(toxic_df))\n",
    "        \n",
    "        ##\n",
    "        # compute the toxicity score\n",
    "        get_results_dict['toxicity_ratio'].append(tree_metrics.get_toxicity_ratio(toxic_df, df))\n",
    "\n",
    "        ##\n",
    "        #building the edge list according to the algorithm presented\n",
    "        vertices = list(set(df.id.tolist() + df.replied_id.tolist()))\n",
    "        edge_list = tree_metrics.create_edge_list(vertices, root_node, df)\n",
    "        gtree = tree_metrics.create_graph(vertices, edge_list)\n",
    "        #setting toxicity values as attribute of the nodes\n",
    "        gtree.vs['toxicity'] = df.toxicity_score.tolist()\n",
    "        \n",
    "        #Tree metrics: size, depth, wiener index, assortativity and average toxicity distance\n",
    "        get_results_dict['size'].append(len(gtree.vs['name']))\n",
    "        get_results_dict['depth'].append(tree_metrics.get_depth(gtree, root_node))\n",
    "        get_results_dict['wiener_index'].append(tree_metrics.get_wiener_index(T = gtree, root_node = root_node))\n",
    "        get_results_dict['assortativity_tox'].append(tree_metrics.get_assortativity(gtree, numeric_prop ='toxicity',directed_flag = False))\n",
    "        get_results_dict['avg_tox_distance'].append(tree_metrics.mean_root_distance(gtree, toxic_df, root_node))\n",
    "        get_results_dict['top_3_annotations'].append(ExtractAnnotations.get_top_annotations_from_thread(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.DataFrame(get_results_dict)\n",
    "xdf.sort_values(by='created_at', inplace=True)\n",
    "#xdf.to_csv('/Users/alessandroquattrociocchi/Git/free-speech-analysis/plots/thesis_data/full_metrics_thesis.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quali sono stati i topic che maggiormente hanno favorito il linguaggio d'odio?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessandroquattrociocchi/Git/free-speech-analysis/code_py/cascades_metrics/annotations_processing.py:30: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  annotations_dict['2020'] = pd.Series(list(itertools.chain.from_iterable(annotations20))).value_counts()[:top_n_annotations].index\n",
      "/Users/alessandroquattrociocchi/Git/free-speech-analysis/code_py/cascades_metrics/annotations_processing.py:34: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  annotations_dict['2021'] = pd.Series(list(itertools.chain.from_iterable(annotations21))).value_counts()[:top_n_annotations].index\n",
      "/Users/alessandroquattrociocchi/Git/free-speech-analysis/code_py/cascades_metrics/annotations_processing.py:38: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  annotations_dict['2022'] = pd.Series(list(itertools.chain.from_iterable(annotations22))).value_counts()[:top_n_annotations].index\n"
     ]
    }
   ],
   "source": [
    "very_low_df = ExtractAnnotations.divide_dataframe_by_score(xdf, score_flag='very_low')\n",
    "\n",
    "top_ann_very_low = ExtractAnnotations.get_annotations_per_year(very_low_df, threshold=0.1, top_n_annotations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2020': Float64Index([], dtype='float64'),\n",
       " '2021': Float64Index([], dtype='float64'),\n",
       " '2022': Float64Index([], dtype='float64')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_ann_very_low"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f7b5a64b057b66b63c6a6b87378565f979ddcf7080d96218b6977087945ee3d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
